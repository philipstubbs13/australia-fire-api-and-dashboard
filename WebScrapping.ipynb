{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the connection to MongoDB\n",
    "# create an instance of the database and collections\n",
    "\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "db = client.australia_fire_db\n",
    "bushfiresbyState = db.bushfiresbyState\n",
    "historicalFires = db.historicalFires\n",
    "aus2019_2020 = db.aus2019_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls to scrape\n",
    "\n",
    "byStateurl = 'https://en.wikipedia.org/wiki/2019%E2%80%9320_Australian_bushfire_season'\n",
    "historicalurl = 'https://en.wikipedia.org/wiki/List_of_major_bushfires_in_Australia'\n",
    "aus2019_2020url = 'https://en.m.wikipedia.org/wiki/List_of_fires_and_impacts_of_the_2019-20_Australian_bushfire_season'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request the html using beautiful soup\n",
    "\n",
    "historical_response = requests.get(historicalurl)\n",
    "bystate_response = requests.get(byStateurl)\n",
    "aus2019_2020_response = requests.get(aus2019_2020url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the html text\n",
    "\n",
    "h_soup = BeautifulSoup(historical_response.text, 'html.parser')\n",
    "bs_soup = BeautifulSoup(bystate_response.text, 'html.parser')\n",
    "a_soup = BeautifulSoup(aus2019_2020_response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Bushfire Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the table headers are within the table body in the first two rows\n",
    "\n",
    "h_table = h_soup.find('table', class_=\"wikitable\")\n",
    "h_table_headers = h_soup.find_all(\"tr\")[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the table headers and append them to the headers list\n",
    "\n",
    "h_headers = []\n",
    "for table_header in h_table_headers:\n",
    "    try:\n",
    "        value = table_header.text\n",
    "#         value = value.split(\"\\n\")\n",
    "        h_headers.append(value)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        \n",
    "print(h_headers)\n",
    "print(len(h_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually create the headers list, because the scraped table headers was too complicated\n",
    "# if there is time, create logic to put the two header rows into one similar to the list below\n",
    "\n",
    "h_headers = ['Date', 'Name', 'State(s)/territories', 'AreaBurned(ha)', 'AreaBurned(acres)', 'Fatalities', 'PropertiesDamaged(HomesDestroyed)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find objects to scrape\n",
    "# the table data is after the headers which is in the first two rows\n",
    "\n",
    "h_table = h_soup.find('table', class_=\"wikitable\")\n",
    "h_table_body = h_table.find(\"tbody\")\n",
    "h_table_row = h_table_body.find_all('tr')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data and create a list of lists for each row of data\n",
    "\n",
    "h_data = []\n",
    "\n",
    "for row in h_table_row:\n",
    "    \n",
    "    datarow = []\n",
    "    \n",
    "    table_data = row.find_all('td')\n",
    "    \n",
    "    for tdata in table_data:\n",
    "        try:\n",
    "            value = tdata.text\n",
    "    #             value.split(\"\\n\\n\")\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            datarow.append(value)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            \n",
    "    h_data.append(datarow)\n",
    "        \n",
    "# print(h_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert scrapped data into a dataframe\n",
    "\n",
    "h_df = pd.DataFrame(h_data)\n",
    "h_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep desired rows\n",
    "\n",
    "h_drop_rows_df = h_df[[0,1,2,3,4,5,6]]\n",
    "h_drop_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add column headers\n",
    "\n",
    "h_drop_rows_df.columns = h_headers\n",
    "h_drop_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Nil with 0 values\n",
    "\n",
    "h_nil_df = h_drop_rows_df.replace('Nil', '0')\n",
    "h_nil_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove commas from numbers\n",
    "# replace unwanted values as NaNs\n",
    "# replace range data into an average value\n",
    "# remove citations found within []\n",
    "\n",
    "h_tonumeric_df = h_nil_df.copy()\n",
    "h_tonumeric_df['AreaBurned(ha)'] = h_nil_df['AreaBurned(ha)'].str.replace(',','')\n",
    "h_tonumeric_df['AreaBurned(acres)'] = h_nil_df['AreaBurned(acres)'].str.replace(',','')\n",
    "h_tonumeric_df['PropertiesDamaged(HomesDestroyed)'] = h_nil_df['PropertiesDamaged(HomesDestroyed)'].str.replace(',','')\n",
    "\n",
    "columnstoedit = ['AreaBurned(ha)','AreaBurned(acres)','Fatalities','PropertiesDamaged(HomesDestroyed)']\n",
    "\n",
    "for i in range(len(h_tonumeric_df['Date'])):\n",
    "    for column in columnstoedit:\n",
    "        \n",
    "        if ((h_tonumeric_df[column][i] == '') and (h_tonumeric_df[column][i] != '0')) or (h_tonumeric_df[column][i] == 'unknown') :\n",
    "            h_tonumeric_df[column][i] = 'NaN'\n",
    "\n",
    "        if 'approx. ' in str(h_tonumeric_df[column][i]):\n",
    "            h_tonumeric_df[column][i] = h_tonumeric_df[column][i].replace('approx. ', '')\n",
    "            \n",
    "        if 'than ' in str(h_tonumeric_df[column][i]):\n",
    "            h_tonumeric_df[column][i] = h_tonumeric_df[column][i].split(' ')[-1]\n",
    "\n",
    "        if len(str(h_tonumeric_df[column][i]).split('–')) == 2:\n",
    "            splitvalues = str(h_tonumeric_df[column][i]).split('–')\n",
    "            h_tonumeric_df[column][i] = np.mean([int(splitvalues[0]), int(splitvalues[1])])\n",
    "\n",
    "        if len(str(h_tonumeric_df[column][i]).split('[')) >= 2:\n",
    "            h_tonumeric_df[column][i] = str(h_tonumeric_df[column][i]).split('[')[0]\n",
    "        \n",
    "\n",
    "h_tonumeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the year the fire took place, if over two years, grab the beginning year\n",
    "\n",
    "h_year_df = h_tonumeric_df.copy()\n",
    "h_year_df['Year'] = ''\n",
    "for i in range(len(h_year_df['Date'])):\n",
    "    \n",
    "    resultslist = []\n",
    "    results = str(h_year_df['Date'][i]).split(' ')\n",
    "\n",
    "    for result in results:\n",
    "        try:\n",
    "            value = int(result)\n",
    "            if len(str(value)) == 4:\n",
    "                resultslist.append(value)\n",
    "                year = np.min(resultslist)\n",
    "                h_year_df['Year'][i] = year\n",
    "\n",
    "        except:\n",
    "            year = 'NaN'\n",
    "            h_year_df['Year'][i] = year\n",
    "    \n",
    "h_year_df[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan values and empty values and convert them to integers\n",
    "\n",
    "h_casting_df = h_year_df.copy()\n",
    "\n",
    "h_casting_df = h_casting_df[h_casting_df['AreaBurned(ha)'] != 'NaN']\n",
    "h_casting_df = h_casting_df[h_casting_df['PropertiesDamaged(HomesDestroyed)'] != 'NaN']\n",
    "h_casting_df = h_casting_df[h_casting_df['PropertiesDamaged(HomesDestroyed)'] != '']\n",
    "h_casting_df = h_casting_df[h_casting_df['Year'] != 'NaN']\n",
    "h_casting_df = h_casting_df.astype(\n",
    "    {\n",
    "        'AreaBurned(ha)':'int',\n",
    "        'AreaBurned(acres)': 'int',\n",
    "        'Fatalities':'int',\n",
    "        'PropertiesDamaged(HomesDestroyed)':'int',\n",
    "        'Year': 'int'\n",
    "    })\n",
    "# h_casting_df['Year'] = pd.to_datetime(h_casting_df['Year'], format = '%Y')\n",
    "h_casting_df.dtypes\n",
    "h_casting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataframe into a dictionary so that we can feed it to MongoDB\n",
    "h_dict = h_casting_df.to_dict('records')\n",
    "h_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert records into the MongoDB collection histroicalFires\n",
    "\n",
    "if (historicalFires.count() == 0):\n",
    "    historicalFires.insert(h_dict)\n",
    "    \n",
    "else:\n",
    "    print(\"Data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Data from 2019-2020 by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the table of interest\n",
    "\n",
    "bs_table = bs_soup.find('table', class_='sortable')\n",
    "print(bs_table.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the column headers\n",
    "\n",
    "bs_theaders = bs_soup.find('table', class_='sortable').find_all('tr')[0:2]\n",
    "\n",
    "\n",
    "bs_headers = []\n",
    "for table_header in bs_theaders:\n",
    "    try:\n",
    "        value = table_header.text\n",
    "        value = value.split(\"\\n\\n\")\n",
    "        bs_headers.append(value)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        \n",
    "print(bs_headers)\n",
    "print(len(bs_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually create the column headers\n",
    "\n",
    "bs_theaders = ['State/Territory', 'Fatalities', 'Homeslost', 'Area(estimated)(ha)', 'Area(estimated)(acres)', 'Notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data and create a list of lists for each row of data\n",
    "\n",
    "bs_table_row = bs_soup.find('table', class_='sortable').find_all('tr')[2:]\n",
    "\n",
    "bs_data = []\n",
    "\n",
    "for row in bs_table_row:\n",
    "    \n",
    "    datarow = []\n",
    "    \n",
    "    table_header = row.find('th').text\n",
    "    table_header = table_header.replace('\\n','')\n",
    "    datarow.append(table_header)\n",
    "    table_data = row.find_all('td')\n",
    "    \n",
    "    for tdata in table_data:\n",
    "        try:\n",
    "            value = tdata.text\n",
    "    #             value.split(\"\\n\\n\")\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            datarow.append(value)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            \n",
    "    bs_data.append(datarow)\n",
    "        \n",
    "print(bs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert scrapped data into a dataframe\n",
    "\n",
    "bs_df = pd.DataFrame(bs_data)\n",
    "bs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column headers\n",
    "\n",
    "bs_df.columns = bs_theaders\n",
    "bs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any unwated characters from the columns\n",
    "\n",
    "bs_dropchar_df = bs_df.drop('Notes', axis=1)\n",
    "\n",
    "columnstoedit = ['Homeslost', 'Area(estimated)(ha)', 'Area(estimated)(acres)']\n",
    "\n",
    "bs_dropchar_df[\"Homeslost\"] = bs_dropchar_df[\"Homeslost\"].replace({\n",
    "    \"3,500+\": \"3500\"\n",
    "})\n",
    "\n",
    "for column in columnstoedit:\n",
    "    for i in range(len(bs_dropchar_df[column])):\n",
    "        try:\n",
    "            bs_dropchar_df[column][i] = str(bs_dropchar_df[column][i]).replace(',','')\n",
    "            bs_dropchar_dfr_df[column][i] = str(bs_dropchar_df[column][i]).replace('+', '')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "bs_dropchar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case the columns of interest\n",
    "\n",
    "bs_cast_df = bs_dropchar_df.copy()\n",
    "\n",
    "bs_cast_df = bs_cast_df.astype({\n",
    "    'Fatalities': 'int',\n",
    "    'Homeslost': 'int',\n",
    "    'Area(estimated)(ha)': 'int',\n",
    "    'Area(estimated)(acres)': 'int'\n",
    "})\n",
    "\n",
    "bs_cast_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of our dataframe\n",
    "\n",
    "bs_dict = bs_cast_df.to_dict('records')\n",
    "bs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data into mongodb\n",
    "\n",
    "if (bushfiresbyState.count() == 0):\n",
    "    bushfiresbyState.insert(bs_dict)\n",
    "    \n",
    "else:\n",
    "    print(\"Data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Australian 2019 - 2020 Bushfire Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the table and the table rows from the site of interest\n",
    "\n",
    "a_table = a_soup.find('table', class_=\"wikitable\")\n",
    "a_table_body = a_table.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all of the table rows (includeing the headers)\n",
    "\n",
    "a_tdata = []\n",
    "for table_header in a_table_body:\n",
    "    data_row = []\n",
    "    \n",
    "    t_headers = table_header.find_all(\"th\")\n",
    "    for header in t_headers:\n",
    "        try:\n",
    "            value = header.text\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            data_row.append(value)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "\n",
    "    t_data = table_header.find_all(\"td\")\n",
    "    for data in t_data:\n",
    "        try:\n",
    "            value = data.text\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            data_row.append(value)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            \n",
    "    a_tdata.append(data_row)\n",
    "        \n",
    "print(a_tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from the scrapped data\n",
    "\n",
    "a_df = pd.DataFrame(a_tdata)\n",
    "a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first row as the column header\n",
    "\n",
    "a_setheaders_df = a_df.copy()\n",
    "a_setheaders_df.columns = a_setheaders_df.iloc[0]\n",
    "a_setheaders_df = a_setheaders_df.drop(a_setheaders_df.index[0])\n",
    "a_setheaders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any unwatnted columns and rename some column headers\n",
    "\n",
    "a_editcols_df = a_setheaders_df.copy()\n",
    "a_editcols_df = a_editcols_df.drop(['Start Date', 'Contained / Cease Date', 'Comments'], axis=1)\n",
    "a_editcols_df = a_editcols_df.rename({'Area impacted': 'AreaImpacted(ha)'}, axis=1)\n",
    "a_editcols_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any unwanted characters from columns\n",
    "\n",
    "a_parsedata_df = a_editcols_df.copy()\n",
    "\n",
    "for i in range(len(a_parsedata_df['Local Government Area(s)'])):\n",
    "    try:\n",
    "\n",
    "        a_parsedata_df['Local Government Area(s)'][i] = str(a_parsedata_df['Local Government Area(s)'][i]).split('[')[0]\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "for i in range(len(a_parsedata_df['AreaImpacted(ha)'])+1):\n",
    "    try:\n",
    "        value = str(a_parsedata_df['AreaImpacted(ha)'][i]).split(' ')[0]\n",
    "        value = value.replace(',','')\n",
    "        a_parsedata_df['AreaImpacted(ha)'][i] = value\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "a_parsedata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any rows that don't have values in the area impacted column\n",
    "\n",
    "a_droprows_df = a_parsedata_df.copy()\n",
    "\n",
    "a_droprows_df = a_droprows_df[a_droprows_df['AreaImpacted(ha)']!='']\n",
    "a_droprows_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the columns of interest\n",
    "\n",
    "a_cast_df = a_droprows_df.copy()\n",
    "a_cast_df = a_cast_df.astype({\n",
    "    'AreaImpacted(ha)': 'int'\n",
    "})\n",
    "a_cast_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the final dataframe into a dictionary\n",
    "\n",
    "a_dict = a_cast_df.to_dict('record')\n",
    "a_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert records into the MongoDB collection histroicalFires\n",
    "\n",
    "if (aus2019_2020.count() == 0):\n",
    "    aus2019_2020.insert(a_dict)\n",
    "    \n",
    "else:\n",
    "    print(\"Data already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
