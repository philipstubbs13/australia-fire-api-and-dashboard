{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the connection to MongoDB\n",
    "# create an instance of the database and collections\n",
    "\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "db = client.australia_fire_db\n",
    "bushfiresbyState = db.bushfiresbyState\n",
    "historicalFires = db.historicalFires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls to scrape\n",
    "\n",
    "byStateurl = 'https://en.wikipedia.org/wiki/2019%E2%80%9320_Australian_bushfire_season'\n",
    "historicalurl = 'https://en.wikipedia.org/wiki/List_of_major_bushfires_in_Australia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request the html using beautiful soup\n",
    "\n",
    "historical_response = requests.get(historicalurl)\n",
    "bystate_response = requests.get(byStateurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the html text\n",
    "\n",
    "h_soup = BeautifulSoup(historical_response.text, 'html.parser')\n",
    "bs_soup = BeautifulSoup(bystate_response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Bushfire Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the table headers are within the table body in the first two rows\n",
    "\n",
    "h_table = h_soup.find('table', class_=\"wikitable\")\n",
    "h_table_headers = h_soup.find_all(\"tr\")[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the table headers and append them to the headers list\n",
    "\n",
    "h_headers = []\n",
    "for table_header in h_table_headers:\n",
    "    try:\n",
    "        value = table_header.text\n",
    "#         value = value.split(\"\\n\")\n",
    "        h_headers.append(value)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        \n",
    "print(h_headers)\n",
    "print(len(h_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually create the headers list, because the scraped table headers was too complicated\n",
    "# if there is time, create logic to put the two header rows into one similar to the list below\n",
    "\n",
    "h_headers = ['Date', 'Name', 'State(s)/territories', 'AreaBurned(ha)', 'AreaBurned(acres)', 'Fatalities', 'PropertiesDamaged(HomesDestroyed)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find objects to scrape\n",
    "# the table data is after the headers which is in the first two rows\n",
    "\n",
    "h_table = h_soup.find('table', class_=\"wikitable\")\n",
    "h_table_body = h_table.find(\"tbody\")\n",
    "h_table_row = h_table_body.find_all('tr')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data and create a list of lists for each row of data\n",
    "\n",
    "h_data = []\n",
    "\n",
    "for row in h_table_row:\n",
    "    \n",
    "    datarow = []\n",
    "    \n",
    "    table_data = row.find_all('td')\n",
    "    \n",
    "    for tdata in table_data:\n",
    "        try:\n",
    "            value = tdata.text\n",
    "    #             value.split(\"\\n\\n\")\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            datarow.append(value)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            \n",
    "    h_data.append(datarow)\n",
    "        \n",
    "# print(h_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert scrapped data into a dataframe\n",
    "\n",
    "h_df = pd.DataFrame(h_data)\n",
    "h_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep desired rows\n",
    "\n",
    "h_drop_rows_df = h_df[[0,1,2,3,4,5,6]]\n",
    "h_drop_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add column headers\n",
    "\n",
    "h_drop_rows_df.columns = h_headers\n",
    "h_drop_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Nil with 0 values\n",
    "\n",
    "h_nil_df = h_drop_rows_df.replace('Nil', '0')\n",
    "h_nil_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove commas from numbers\n",
    "# replace unwanted values as NaNs\n",
    "# replace range data into an average value\n",
    "# remove citations found within []\n",
    "\n",
    "h_tonumeric_df = h_nil_df.copy()\n",
    "h_tonumeric_df['AreaBurned(ha)'] = h_nil_df['AreaBurned(ha)'].str.replace(',','')\n",
    "h_tonumeric_df['AreaBurned(acres)'] = h_nil_df['AreaBurned(acres)'].str.replace(',','')\n",
    "h_tonumeric_df['PropertiesDamaged(HomesDestroyed)'] = h_nil_df['PropertiesDamaged(HomesDestroyed)'].str.replace(',','')\n",
    "\n",
    "columnstoedit = ['AreaBurned(ha)','AreaBurned(acres)','Fatalities','PropertiesDamaged(HomesDestroyed)']\n",
    "\n",
    "for i in range(len(h_tonumeric_df['Date'])):\n",
    "    for column in columnstoedit:\n",
    "        \n",
    "        if ((h_tonumeric_df[column][i] == '') and (h_tonumeric_df[column][i] != '0')) or (h_tonumeric_df[column][i] == 'unknown') :\n",
    "            h_tonumeric_df[column][i] = 'NaN'\n",
    "\n",
    "        if 'approx. ' in str(h_tonumeric_df[column][i]):\n",
    "            h_tonumeric_df[column][i] = h_tonumeric_df[column][i].replace('approx. ', '')\n",
    "            \n",
    "        if 'than ' in str(h_tonumeric_df[column][i]):\n",
    "            h_tonumeric_df[column][i] = h_tonumeric_df[column][i].split(' ')[-1]\n",
    "\n",
    "        if len(str(h_tonumeric_df[column][i]).split('–')) == 2:\n",
    "            splitvalues = str(h_tonumeric_df[column][i]).split('–')\n",
    "            h_tonumeric_df[column][i] = np.mean([int(splitvalues[0]), int(splitvalues[1])])\n",
    "\n",
    "        if len(str(h_tonumeric_df[column][i]).split('[')) >= 2:\n",
    "            h_tonumeric_df[column][i] = str(h_tonumeric_df[column][i]).split('[')[0]\n",
    "        \n",
    "\n",
    "h_tonumeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the year the fire took place, if over two years, grab the beginning year\n",
    "\n",
    "h_year_df = h_tonumeric_df.copy()\n",
    "h_year_df['Year'] = ''\n",
    "for i in range(len(h_year_df['Date'])):\n",
    "    \n",
    "    resultslist = []\n",
    "    results = str(h_year_df['Date'][i]).split(' ')\n",
    "\n",
    "    for result in results:\n",
    "        try:\n",
    "            value = int(result)\n",
    "            if len(str(value)) == 4:\n",
    "                resultslist.append(value)\n",
    "                year = np.min(resultslist)\n",
    "\n",
    "        except:\n",
    "            year = 'NaN'\n",
    "\n",
    "        h_year_df['Year'][i] = year\n",
    "    \n",
    "h_year_df[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan values and empty values and convert them to integers\n",
    "\n",
    "h_casting_df = h_year_df.copy()\n",
    "\n",
    "h_casting_df = h_casting_df[h_casting_df['AreaBurned(ha)'] != 'NaN']\n",
    "h_casting_df = h_casting_df[h_casting_df['PropertiesDamaged(HomesDestroyed)'] != 'NaN']\n",
    "h_casting_df = h_casting_df[h_casting_df['PropertiesDamaged(HomesDestroyed)'] != '']\n",
    "h_casting_df = h_casting_df[h_casting_df['Year'] != 'NaN']\n",
    "h_casting_df = h_casting_df.astype(\n",
    "    {\n",
    "        'AreaBurned(ha)':'int',\n",
    "        'AreaBurned(acres)': 'int',\n",
    "        'Fatalities':'int',\n",
    "        'PropertiesDamaged(HomesDestroyed)':'int',\n",
    "        'Year': 'int'\n",
    "    })\n",
    "# h_casting_df['Year'] = pd.to_datetime(h_casting_df['Year'], format = '%Y')\n",
    "h_casting_df.dtypes\n",
    "h_casting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataframe into a dictionary so that we can feed it to MongoDB\n",
    "h_dict = h_casting_df.to_dict('records')\n",
    "h_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert records into the MongoDB collection histroicalFires\n",
    "\n",
    "if (historicalFires.count() == 0):\n",
    "    historicalFires.insert(h_dict)\n",
    "    \n",
    "else:\n",
    "    print(\"Data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Data from 2019-2020 by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_table = bs_soup.find('table', class_='sortable')\n",
    "print(bs_table.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_theaders = bs_soup.find('table', class_='sortable').find_all('tr')[0:2]\n",
    "\n",
    "\n",
    "bs_headers = []\n",
    "for table_header in bs_theaders:\n",
    "    try:\n",
    "        value = table_header.text\n",
    "        value = value.split(\"\\n\\n\")\n",
    "        bs_headers.append(value)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        \n",
    "print(bs_headers)\n",
    "print(len(bs_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_theaders = ['State/Territory', 'Fatalities', 'Homeslost', 'Area(estimated)(ha)', 'Area(estimated)(acres)', 'Notes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data and create a list of lists for each row of data\n",
    "\n",
    "bs_table_row = bs_soup.find('table', class_='sortable').find_all('tr')[2:]\n",
    "\n",
    "bs_data = []\n",
    "\n",
    "for row in bs_table_row:\n",
    "    \n",
    "    datarow = []\n",
    "    \n",
    "    table_header = row.find('th').text\n",
    "    table_header = table_header.replace('\\n','')\n",
    "    datarow.append(table_header)\n",
    "    table_data = row.find_all('td')\n",
    "    \n",
    "    for tdata in table_data:\n",
    "        try:\n",
    "            value = tdata.text\n",
    "    #             value.split(\"\\n\\n\")\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            datarow.append(value)\n",
    "\n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "            \n",
    "    bs_data.append(datarow)\n",
    "        \n",
    "print(bs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert scrapped data into a dataframe\n",
    "\n",
    "bs_df = pd.DataFrame(bs_data)\n",
    "bs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column headers\n",
    "\n",
    "bs_df.columns = bs_theaders\n",
    "bs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_dropchar_df = bs_df.drop('Notes', axis=1)\n",
    "\n",
    "columnstoedit = ['Homeslost', 'Area(estimated)(ha)', 'Area(estimated)(acres)']\n",
    "\n",
    "for column in columnstoedit:\n",
    "    for i in range(len(bs_dropchar_df[column])):\n",
    "        try:\n",
    "            bs_dropchar_df[column][i] = str(bs_dropchar_df[column][i]).replace(',','')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "bs_dropchar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
